Sum of even numbers program :
 l = list(range(1,100001)) # this is list
 type(l)
 len(l)
 help(sc.parallelize) #parallelize convert collections in to RDD.
 lRDD = sc.parallelize()
 type(lRDD)
 lRDD.count() ### Actions # get the length of RDD
 lRDD.first() ### Actions  # return first  row
 lRDD.take(10) ### Actions  # return 10  row
 lRDD.collect() ## convert RDD into collections ## this is not recomnded to use beacuase it take more time and memory
 
 lEVEN = lRDD.filter(lambda x : x % 2 == 0)
 type(lEVEN)
 lEVEN.count()
 lEVEN.reduce(lambda x,y: x+y) 
 # minimum   lEVEN.reduce(lambda x,y: x if (x<y) else y) 
  # max   lEVEN.reduce(lambda x,y: x if (x>y) else y) 
   OR
  alternate for reduce lambda fuction:
  from operator import add
  lEVEN.reduce(add)
  
  Word count program:
  
 lines = sc.textFile("path")
 ## flatMap() ## it is used to when  desired output record is greater than input record 
 ## Map() ## it is used to when  desired output record is equal to input record
 words = lines.flatMap(lambda w: w.split(" ") )
 words.count()
 wordTuple = words.map(lambda w: (w, 1))
 wordCount = wordTuple.reduceByKey(add)
 wordCount.count()
 for i in wordCount.take(10): print(i)
 
 groupByKey: take the input data in the form of tuple and group the data baed on key and retun the output as tuple (key, [it of value])
 reduceByKey: take the input data in the form of tuple and a lambda function 
 aggregateByKey: take the input data in the form of tuple and two lambda function 
 
 distinct(): will you give you distinct values : orderr.map(lambda x: x.split(',')[3]).distinct().collect()
 lambda fucntion login should in pure python.
 
 takeSample(True, 10):  it will give you 10 random records.
 
 Join(): it takes input in the form of tuple and then output in the for of tuple
 targetRDD = firstRDD.join(secondRDD) ##firstRDD  = (k,v), secondRDD  = (k,w), targetRDD  = (k,(v,w))
 
 sortByKey(): Use to sort the data. Bydefault data will be sorted in accesending order on Key (K,V)
 RDD.sortByKey()
 RDD.sortByKey(False) # To sort in decending order
 
 mapPartitions():  It is efficent than map() beacuse it process that one pattion data a data intstead of reading the data row by row just like in case of map function.
                   Beacuse of this it reduce the time for each db connection and reduce the overall execution time.
                   lines.mapPartitions(lambda i : getWordTuples(i)) # here i is of iterable data. 
                   
                   getWordTuples(i):
                      import itertools as it
                      wordTuples =  map(lambda s: (s,1), it.chain.from_iterable(map(lambda s: s.split(" "),i)))
                      return wordTuples
                      
 To get top N products by pricecategory:
 
 products = sc.textFile("path")
 productsFiltered = products.filter(lambda p: p.split(",")[4] != "")
 productsMap = productsFiltered.map(lambda p: int(p.split(",")[1]), p)
 productsGBCategory = productsMap.groupBy()

 def getTopNPricesProducts(products,topN):
     import itertools as it
     productPrices = sorted(set(map(lambda p: float(p.split(",")[4]), products)),reverse =  True)[:topN]
     productSorted = sorted(products, key=lambda p: float(p.split(",")[4]), reverse =  True)
     return it.takeWhile(lambda p: float(p.split(",")[4]) in productPrices, productSorted)
     
topNPricesProductsByCategory = productsGBCategory.flatMap(lambda p : getTopNPricesProducts(list(p[1]),4))

map, filter,functools.reduce, itertools.takewhile,itertools.chain, pandas ....need to explore



